{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from zipfile import ZipFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing neural network libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D, Concatenate, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "validation_tweets = []\n",
    "test_tweets = []\n",
    "\n",
    "for line in open(r'C:\\Users\\Dell\\Desktop\\train.json','r'):\n",
    "    train_tweets.append(json.loads(line))\n",
    "    \n",
    "for line in open(r'C:\\Users\\Dell\\Desktop\\val.json','r'):\n",
    "    validation_tweets.append(json.loads(line))\n",
    "\n",
    "for line in open(r'C:\\Users\\Dell\\Desktop\\test.json', 'r'):\n",
    "    test_tweets.append(json.loads(line))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_tweets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data)\n",
    "validation_df = pd.DataFrame(validation_tweets)\n",
    "test_df = pd.DataFrame(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim', 'id', 'json_file_id', 'justification', 'label', 'originator',\n",
       "       'party', 'title', 'topics'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10269"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swc_y_train = train_data['label']\n",
    "# swc_y_validation = validation_df['label']\n",
    "# swc_y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swc_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "train_data['new'] = train_data['claim'].map(str) + train_data['justification'].map(str)\n",
    "for i in range(len(train_data['label'])):\n",
    "    train_data['label'].replace(['mostly-true','half-true'], 'true', regex=True, inplace=True)\n",
    "    train_data['label'].replace(['barely-true','pants-fire'], 'false', regex=True, inplace=True)\n",
    "    \n",
    "y_train = pd.get_dummies(train_data['label'], drop_first=True)\n",
    "\n",
    "#validation data \n",
    "for i in range(len(validation_df['label'])):\n",
    "    validation_df['label'].replace(['mostly-true','half-true'], 'true', regex=True, inplace=True)\n",
    "    validation_df['label'].replace(['barely-true','pants-fire'], 'false', regex=True, inplace=True)\n",
    "\n",
    "y_validation = pd.get_dummies(validation_df['label'], drop_first=True)\n",
    "\n",
    "#test data\n",
    "for i in range(len(test_df['label'])):\n",
    "    test_df['label'].replace(['mostly-true','half-true'], 'true', regex=True, inplace=True)\n",
    "    test_df['label'].replace(['barely-true','pants-fire'], 'false', regex=True, inplace=True)\n",
    "    \n",
    "y_test = pd.get_dummies(test_df['label'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining claim and justification\n",
    "train_data['new'] = train_data['claim'].map(str) + train_data['justification'].map(str)\n",
    "validation_df['new'] = validation_df['claim'].map(str) + validation_df['justification'].map(str)\n",
    "test_df['new'] = test_df['claim'].map(str) + test_df['justification'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim', 'id', 'json_file_id', 'justification', 'label', 'originator',\n",
       "       'party', 'title', 'topics', 'new'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10269"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>id</th>\n",
       "      <th>json_file_id</th>\n",
       "      <th>justification</th>\n",
       "      <th>label</th>\n",
       "      <th>originator</th>\n",
       "      <th>party</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "      <th>new</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2635.json</td>\n",
       "      <td>That's a premise that he fails to back up. Ann...</td>\n",
       "      <td>false</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>Texas</td>\n",
       "      <td>State representative</td>\n",
       "      <td>[abortion]</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>1</td>\n",
       "      <td>10540.json</td>\n",
       "      <td>\"Surovell said the decline of coal \"\"started w...</td>\n",
       "      <td>true</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>[energy, history, job-accomplishments]</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Hillary Clinton agrees with John McCain \"\"by ...</td>\n",
       "      <td>2</td>\n",
       "      <td>324.json</td>\n",
       "      <td>\"Obama said he would have voted against the am...</td>\n",
       "      <td>true</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>President</td>\n",
       "      <td>[foreign-policy]</td>\n",
       "      <td>\"Hillary Clinton agrees with John McCain \"\"by ...</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>3</td>\n",
       "      <td>1123.json</td>\n",
       "      <td>\"The release may have a point that Mikulskis c...</td>\n",
       "      <td>false</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[health-care]</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>4</td>\n",
       "      <td>9028.json</td>\n",
       "      <td>\"Crist said that the economic \"\"turnaround sta...</td>\n",
       "      <td>true</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>Florida</td>\n",
       "      <td></td>\n",
       "      <td>[economy, jobs]</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  id json_file_id  \\\n",
       "0  Says the Annies List political group supports ...   0    2635.json   \n",
       "1  When did the decline of coal start? It started...   1   10540.json   \n",
       "2  \"Hillary Clinton agrees with John McCain \"\"by ...   2     324.json   \n",
       "3  Health care reform legislation is likely to ma...   3    1123.json   \n",
       "4  The economic turnaround started at the end of ...   4    9028.json   \n",
       "\n",
       "                                       justification  label      originator  \\\n",
       "0  That's a premise that he fails to back up. Ann...  false    dwayne-bohac   \n",
       "1  \"Surovell said the decline of coal \"\"started w...   true  scott-surovell   \n",
       "2  \"Obama said he would have voted against the am...   true    barack-obama   \n",
       "3  \"The release may have a point that Mikulskis c...  false    blog-posting   \n",
       "4  \"Crist said that the economic \"\"turnaround sta...   true   charlie-crist   \n",
       "\n",
       "      party                 title                                  topics  \\\n",
       "0     Texas  State representative                              [abortion]   \n",
       "1  Virginia        State delegate  [energy, history, job-accomplishments]   \n",
       "2  Illinois             President                        [foreign-policy]   \n",
       "3                                                           [health-care]   \n",
       "4   Florida                                               [economy, jobs]   \n",
       "\n",
       "                                                 new  length  \n",
       "0  Says the Annies List political group supports ...     331  \n",
       "1  When did the decline of coal start? It started...     783  \n",
       "2  \"Hillary Clinton agrees with John McCain \"\"by ...     521  \n",
       "3  Health care reform legislation is likely to ma...     711  \n",
       "4  The economic turnaround started at the end of ...     618  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = []\n",
    "for text in train_data['new']:\n",
    "    length.append(len(str(text))) \n",
    "train_data['length'] = length\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 9579, 535)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZING FOR CLAIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(texts = train_data['claim'])\n",
    "X_claim = tokenizer.texts_to_sequences(texts = train_data['claim'])\n",
    "X_claim = pad_sequences(sequences = X_claim, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#validation data\n",
    "X_claim_validation = tokenizer.texts_to_sequences(texts = validation_df['claim'])\n",
    "X_claim_validation = pad_sequences(sequences = X_claim_validation, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#testing data\n",
    "X_claim_test = tokenizer.texts_to_sequences(texts = test_df['claim'])\n",
    "X_claim_test = pad_sequences(sequences = X_claim_test, maxlen = max_features, padding = 'pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10269, 520)\n",
      "(10269, 1)\n",
      "(1284, 520)\n",
      "(1284, 1)\n",
      "(1283, 520)\n",
      "(1283, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_claim.shape)\n",
    "print(y_train.shape)\n",
    "print(X_claim_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(X_claim_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM model\n",
    "\n",
    "bilstm_model = Sequential(name = 'lstm_nn_model')\n",
    "bilstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
    "bilstm_model.add(layer = Bidirectional(LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer')))\n",
    "bilstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
    "bilstm_model.add(layer = Dense(units = 100,  activation = 'softmax', name = '4th_layer'))\n",
    "bilstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
    "bilstm_model.add(layer = Dense(units = len(set(y_train)),  activation = 'sigmoid', name = 'output_layer'))\n",
    "# compiling the model\n",
    "bilstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 1602s 11s/step - loss: 0.6870 - accuracy: 0.5626 - val_loss: 0.6885 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "lstm_model_val_lstm_model_fit = bilstm_model.fit(X_claim, y_train,validation_split=0.1, batch_size= 64,epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 21s 503ms/step - loss: 0.6946 - accuracy: 0.5202\n",
      "Test set\n",
      "  Loss: 0.695\n",
      "  Accuracy: 0.520\n",
      "41/41 [==============================] - 21s 510ms/step - loss: 0.6845 - accuracy: 0.5666\n",
      "Test set\n",
      "  Loss: 0.684\n",
      "  Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "#bidirectional\n",
    "accr1 = bilstm_model.evaluate(X_claim_validation,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = bilstm_model.evaluate(X_claim_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model\n",
    "\n",
    "lstm_model = Sequential(name = 'lstm_nn_model')\n",
    "lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
    "lstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\n",
    "lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
    "lstm_model.add(layer = Dense(units = 100,  activation = 'softmax', name = '4th_layer'))\n",
    "lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
    "lstm_model.add(layer = Dense(units = len(set(y_train)),  activation = 'sigmoid', name = 'output_layer'))\n",
    "# compiling the model\n",
    "lstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 484s 3s/step - loss: 0.6866 - accuracy: 0.5633 - val_loss: 0.6887 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "claim_val_lstm_model_fit = lstm_model.fit(X_claim, y_train,validation_split=0.1, batch_size= 64,epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 8s 188ms/step - loss: 0.6952 - accuracy: 0.5202\n",
      "Test set\n",
      "  Loss: 0.695\n",
      "  Accuracy: 0.520\n",
      "41/41 [==============================] - 8s 184ms/step - loss: 0.6844 - accuracy: 0.5666\n",
      "Test set\n",
      "  Loss: 0.684\n",
      "  Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "accr1 = lstm_model.evaluate(X_claim_validation,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = lstm_model.evaluate(X_claim_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF FOR CLAIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for claims\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "claim_train_corpus = []\n",
    "for i in range(0, len(train_data['claim'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", train_data['claim'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    claim_train_corpus.append(new_data)\n",
    "    \n",
    "claim_val_corpus = []\n",
    "for i in range(0, len(validation_df['claim'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", validation_df['claim'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    claim_val_corpus.append(new_data)\n",
    "    \n",
    "claim_test_corpus = []\n",
    "for i in range(0, len(test_df['claim'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", test_df['claim'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    claim_test_corpus.append(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "vectorizer = TfidfVectorizer(max_features = 700)\n",
    "tfidf_x_train = vectorizer.fit_transform(claim_train_corpus).toarray()\n",
    "tfidf_x_train = pad_sequences(sequences = tfidf_x_train, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#tfidf validation data\n",
    "tfidf_x_val = vectorizer.transform(claim_val_corpus).toarray()\n",
    "tfidf_x_val = pad_sequences(sequences = tfidf_x_val, maxlen = max_features, padding = 'pre')\n",
    "# tfidf_claim_val_lstm_model_fit = lstm_model.fit(tfidf_x_train, y_train,\n",
    "#                                                 validation_data = (tfidf_x_val,y_validation), epochs = 1)\n",
    "\n",
    "#tfidf test data\n",
    "tfidf_x_test = vectorizer.transform(claim_test_corpus).toarray()\n",
    "tfidf_x_test = pad_sequences(sequences = tfidf_x_test, maxlen = max_features, padding = 'pre')\n",
    "#tfidf_claim_test_lstm_model_fit = lstm_model.fit(tfidf_x_train, y_train,\n",
    "#                                                validation_data = (tfidf_x_test,y_test), epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 413s 3s/step - loss: 0.6872 - accuracy: 0.5632 - val_loss: 0.6885 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "tfidf_claim_val_lstm_model_fit = lstm_model.fit(tfidf_x_train, y_train,validation_split=0.1, batch_size= 64,epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 7s 161ms/step - loss: 0.6943 - accuracy: 0.5202\n",
      "Test set\n",
      "  Loss: 0.694\n",
      "  Accuracy: 0.520\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 0.6847 - accuracy: 0.5666\n",
      "Test set\n",
      "  Loss: 0.685\n",
      "  Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "accr1 = lstm_model.evaluate(tfidf_x_val,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = lstm_model.evaluate(tfidf_x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 2085s 14s/step - loss: 0.6856 - accuracy: 0.5636 - val_loss: 0.6890 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "#bidirectional\n",
    "\n",
    "lstm_model_val_lstm_model_fit = bilstm_model.fit(tfidf_x_train, y_train,validation_split=0.1, batch_size= 64,epochs = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZE FOR NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(texts = train_data['new'])\n",
    "X_train = tokenizer.texts_to_sequences(texts = train_data['new'])\n",
    "X_train = pad_sequences(sequences = X_train, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#validation\n",
    "X_validation = tokenizer.texts_to_sequences(texts = validation_df['new'])\n",
    "X_validation = pad_sequences(sequences = X_validation, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#test\n",
    "test_text = tokenizer.texts_to_sequences(texts = test_df['new'])\n",
    "test_text = pad_sequences(sequences = test_text, maxlen = max_features, padding = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "925/925 [==============================] - 18386s 20s/step - loss: 0.6859 - accuracy: 0.5636 - val_loss: 0.6887 - val_accuracy: 0.5482\n",
      "Epoch 2/3\n",
      "925/925 [==============================] - 1424s 2s/step - loss: 0.6854 - accuracy: 0.5636 - val_loss: 0.6888 - val_accuracy: 0.5482\n",
      "Epoch 3/3\n",
      "925/925 [==============================] - 1451s 2s/step - loss: 0.6853 - accuracy: 0.5636 - val_loss: 0.6900 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "tfidf_claim_val_lstm_model_fit = lstm_model.fit(X_train, y_train,validation_split=0.1, batch_size= 10,epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 10s 249ms/step - loss: 0.6977 - accuracy: 0.5202\n",
      "Test set\n",
      "  Loss: 0.698\n",
      "  Accuracy: 0.520\n",
      "41/41 [==============================] - 10s 249ms/step - loss: 0.6839 - accuracy: 0.5666\n",
      "Test set\n",
      "  Loss: 0.684\n",
      "  Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "accr1 = lstm_model.evaluate(X_validation,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = lstm_model.evaluate(test_text,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_lstm_model_fit = lstm_model.fit(X_train, y_train, validation_data = (X_validation,y_validation), epochs = 1)\n",
    "# test_lstm_model_fit = lstm_model.fit(X_train, y_train, validation_data = (test_text,y_test), epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_lstm_model_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-2604f7f864ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_lstm_model_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_lstm_model_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_lstm_model_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_lstm_model_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_lstm_model_fit' is not defined"
     ]
    }
   ],
   "source": [
    "# print(val_lstm_model_fit.history['accuracy'])\n",
    "# print(val_lstm_model_fit.history['loss'])\n",
    "# print(val_lstm_model_fit.history['val_accuracy'])\n",
    "# print(val_lstm_model_fit.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF FOR NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf for new\n",
    "new_train_corpus = []\n",
    "for i in range(0, len(train_data['new'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", train_data['new'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    new_train_corpus.append(new_data)\n",
    "    \n",
    "new_validation_corpus = []\n",
    "for i in range(0, len(validation_df['new'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", validation_df['new'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    new_validation_corpus.append(new_data)\n",
    "    \n",
    "new_test_corpus = []\n",
    "for i in range(0, len(test_df['new'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", test_df['new'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    new_test_corpus.append(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "vectorizer = TfidfVectorizer(max_features = 700)\n",
    "tfidf_x_train_new = vectorizer.fit_transform(new_train_corpus).toarray()\n",
    "tfidf_x_train_new = pad_sequences(sequences = tfidf_x_train_new, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#tfidf validation data\n",
    "tfidf_x_val_new = vectorizer.transform(new_validation_corpus).toarray()\n",
    "tfidf_x_val_new = pad_sequences(sequences = tfidf_x_val_new, maxlen = max_features, padding = 'pre')\n",
    "#tfidf_new_val_lstm_model_fit = lstm_model.fit(tfidf_x_train_new, y_train,\n",
    "#                                               validation_data = (tfidf_x_val_new,y_validation), epochs = 1)\n",
    "\n",
    "#tfidf test data\n",
    "tfidf_x_test_new = vectorizer.transform(new_test_corpus).toarray()\n",
    "tfidf_x_test_new = pad_sequences(sequences = tfidf_x_test_new, maxlen = max_features, padding = 'pre')\n",
    "#tfidf_new_test_lstm_model_fit = lstm_model.fit(tfidf_x_train_new, y_train,\n",
    "#                                                validation_data = (tfidf_x_test_new,y_test), epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "925/925 [==============================] - 1186s 1s/step - loss: 0.6856 - accuracy: 0.5636 - val_loss: 0.6892 - val_accuracy: 0.5482\n",
      "Epoch 2/3\n",
      "925/925 [==============================] - 4836s 5s/step - loss: 0.6857 - accuracy: 0.5636 - val_loss: 0.6892 - val_accuracy: 0.5482\n",
      "Epoch 3/3\n",
      "925/925 [==============================] - 1030s 1s/step - loss: 0.6853 - accuracy: 0.5636 - val_loss: 0.6885 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "tfidf_claim_val_lstm_model_fit = lstm_model.fit(tfidf_x_train_new, y_train,validation_split=0.1, batch_size= 10,epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 6s 154ms/step - loss: 0.6942 - accuracy: 0.5202\n",
      "Test set\n",
      "  Loss: 0.694\n",
      "  Accuracy: 0.520\n",
      "41/41 [==============================] - 6s 157ms/step - loss: 0.6848 - accuracy: 0.5666\n",
      "Test set\n",
      "  Loss: 0.685\n",
      "  Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "accr1 = lstm_model.evaluate(tfidf_x_val_new,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = lstm_model.evaluate(tfidf_x_test_new,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
