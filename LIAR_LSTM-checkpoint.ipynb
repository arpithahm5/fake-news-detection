{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from zipfile import ZipFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing neural network libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, LSTM, RNN, SpatialDropout1D, Concatenate, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "validation_tweets = []\n",
    "test_tweets = []\n",
    "\n",
    "for line in open(r'C:\\Users\\Dell\\Desktop\\train.json','r'):\n",
    "    train_tweets.append(json.loads(line))\n",
    "    \n",
    "for line in open(r'C:\\Users\\Dell\\Desktop\\val.json','r'):\n",
    "    validation_tweets.append(json.loads(line))\n",
    "\n",
    "for line in open(r'C:\\Users\\Dell\\Desktop\\test.json', 'r'):\n",
    "    test_tweets.append(json.loads(line))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_tweets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data)\n",
    "validation_df = pd.DataFrame(validation_tweets)\n",
    "test_df = pd.DataFrame(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim', 'id', 'json_file_id', 'justification', 'label', 'originator',\n",
       "       'party', 'title', 'topics'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10269"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swc_y_train = train_data['label']\n",
    "# swc_y_validation = validation_df['label']\n",
    "# swc_y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        false\n",
       "1         true\n",
       "2         true\n",
       "3        false\n",
       "4         true\n",
       "5         true\n",
       "6        false\n",
       "7         true\n",
       "8         true\n",
       "9         true\n",
       "10        true\n",
       "11        true\n",
       "12       false\n",
       "13        true\n",
       "14       false\n",
       "15        true\n",
       "16        true\n",
       "17       false\n",
       "18        true\n",
       "19        true\n",
       "20       false\n",
       "21        true\n",
       "22        true\n",
       "23        true\n",
       "24       false\n",
       "25       false\n",
       "26        true\n",
       "27        true\n",
       "28       false\n",
       "29        true\n",
       "         ...  \n",
       "10239     true\n",
       "10240     true\n",
       "10241     true\n",
       "10242    false\n",
       "10243    false\n",
       "10244    false\n",
       "10245    false\n",
       "10246     true\n",
       "10247    false\n",
       "10248     true\n",
       "10249     true\n",
       "10250    false\n",
       "10251     true\n",
       "10252     true\n",
       "10253    false\n",
       "10254     true\n",
       "10255     true\n",
       "10256    false\n",
       "10257    false\n",
       "10258     true\n",
       "10259    false\n",
       "10260     true\n",
       "10261     true\n",
       "10262    false\n",
       "10263     true\n",
       "10264     true\n",
       "10265     true\n",
       "10266     true\n",
       "10267    false\n",
       "10268    false\n",
       "Name: label, Length: 10269, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# swc_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "train_data['new'] = train_data['claim'].map(str) + train_data['justification'].map(str)\n",
    "for i in range(len(train_data['label'])):\n",
    "    train_data['label'].replace(['mostly-true','half-true'], 'true', regex=True, inplace=True)\n",
    "for i in range(len(train_data['label'])):\n",
    "    train_data['label'].replace(['barely-true','pants-fire'], 'false', regex=True, inplace=True)\n",
    "    \n",
    "y_train = pd.get_dummies(train_data['label'], drop_first=True)\n",
    "\n",
    "#validation data \n",
    "for i in range(len(validation_df['label'])):\n",
    "    validation_df['label'].replace(['mostly-true','half-true'], 'true', regex=True, inplace=True)\n",
    "for i in range(len(validation_df['label'])):\n",
    "    validation_df['label'].replace(['barely-true','pants-fire'], 'false', regex=True, inplace=True)\n",
    "\n",
    "y_validation = pd.get_dummies(validation_df['label'], drop_first=True)\n",
    "\n",
    "#test data\n",
    "for i in range(len(test_df['label'])):\n",
    "    test_df['label'].replace(['mostly-true','half-true'], 'true', regex=True, inplace=True)\n",
    "for i in range(len(test_df['label'])):\n",
    "    test_df['label'].replace(['barely-true','pants-fire'], 'false', regex=True, inplace=True)\n",
    "    \n",
    "y_test = pd.get_dummies(test_df['label'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swc_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['new'] = train_data['claim'].map(str) + train_data['justification'].map(str)\n",
    "validation_df['new'] = validation_df['claim'].map(str) + validation_df['justification'].map(str)\n",
    "test_df['new'] = test_df['claim'].map(str) + test_df['justification'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim', 'id', 'json_file_id', 'justification', 'label', 'originator',\n",
       "       'party', 'title', 'topics', 'new'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10269"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>id</th>\n",
       "      <th>json_file_id</th>\n",
       "      <th>justification</th>\n",
       "      <th>label</th>\n",
       "      <th>originator</th>\n",
       "      <th>party</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "      <th>new</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2635.json</td>\n",
       "      <td>That's a premise that he fails to back up. Ann...</td>\n",
       "      <td>false</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>Texas</td>\n",
       "      <td>State representative</td>\n",
       "      <td>[abortion]</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>1</td>\n",
       "      <td>10540.json</td>\n",
       "      <td>\"Surovell said the decline of coal \"\"started w...</td>\n",
       "      <td>true</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>[energy, history, job-accomplishments]</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Hillary Clinton agrees with John McCain \"\"by ...</td>\n",
       "      <td>2</td>\n",
       "      <td>324.json</td>\n",
       "      <td>\"Obama said he would have voted against the am...</td>\n",
       "      <td>true</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>President</td>\n",
       "      <td>[foreign-policy]</td>\n",
       "      <td>\"Hillary Clinton agrees with John McCain \"\"by ...</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>3</td>\n",
       "      <td>1123.json</td>\n",
       "      <td>\"The release may have a point that Mikulskis c...</td>\n",
       "      <td>false</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[health-care]</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>4</td>\n",
       "      <td>9028.json</td>\n",
       "      <td>\"Crist said that the economic \"\"turnaround sta...</td>\n",
       "      <td>true</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>Florida</td>\n",
       "      <td></td>\n",
       "      <td>[economy, jobs]</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  id json_file_id  \\\n",
       "0  Says the Annies List political group supports ...   0    2635.json   \n",
       "1  When did the decline of coal start? It started...   1   10540.json   \n",
       "2  \"Hillary Clinton agrees with John McCain \"\"by ...   2     324.json   \n",
       "3  Health care reform legislation is likely to ma...   3    1123.json   \n",
       "4  The economic turnaround started at the end of ...   4    9028.json   \n",
       "\n",
       "                                       justification  label      originator  \\\n",
       "0  That's a premise that he fails to back up. Ann...  false    dwayne-bohac   \n",
       "1  \"Surovell said the decline of coal \"\"started w...   true  scott-surovell   \n",
       "2  \"Obama said he would have voted against the am...   true    barack-obama   \n",
       "3  \"The release may have a point that Mikulskis c...  false    blog-posting   \n",
       "4  \"Crist said that the economic \"\"turnaround sta...   true   charlie-crist   \n",
       "\n",
       "      party                 title                                  topics  \\\n",
       "0     Texas  State representative                              [abortion]   \n",
       "1  Virginia        State delegate  [energy, history, job-accomplishments]   \n",
       "2  Illinois             President                        [foreign-policy]   \n",
       "3                                                           [health-care]   \n",
       "4   Florida                                               [economy, jobs]   \n",
       "\n",
       "                                                 new  length  \n",
       "0  Says the Annies List political group supports ...     331  \n",
       "1  When did the decline of coal start? It started...     783  \n",
       "2  \"Hillary Clinton agrees with John McCain \"\"by ...     521  \n",
       "3  Health care reform legislation is likely to ma...     711  \n",
       "4  The economic turnaround started at the end of ...     618  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = []\n",
    "for text in train_data['new']:\n",
    "    length.append(len(str(text))) \n",
    "train_data['length'] = length\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 9579, 535)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(train_data['length']), max(train_data['length']), round(sum(train_data['length'])/len(train_data['length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZING FOR CLAIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(texts = train_data['claim'])\n",
    "X_claim = tokenizer.texts_to_sequences(texts = train_data['claim'])\n",
    "X_claim = pad_sequences(sequences = X_claim, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#validation data\n",
    "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(texts = validation_df['claim'])\n",
    "X_claim_validation = tokenizer.texts_to_sequences(texts = validation_df['claim'])\n",
    "X_claim_validation = pad_sequences(sequences = X_claim_validation, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#testing data\n",
    "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(texts = test_df['claim'])\n",
    "X_claim_test = tokenizer.texts_to_sequences(texts = test_df['claim'])\n",
    "X_claim_test = pad_sequences(sequences = X_claim_test, maxlen = max_features, padding = 'pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10269, 520)\n",
      "(10269, 1)\n",
      "(1284, 520)\n",
      "(1284, 1)\n",
      "(1283, 520)\n",
      "(1283, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_claim.shape)\n",
    "print(y_train.shape)\n",
    "print(X_claim_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(X_claim_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BiLSTM model\n",
    "\n",
    "bilstm_model = Sequential(name = 'lstm_nn_model')\n",
    "bilstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
    "bilstm_model.add(layer = Bidirectional(LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer')))\n",
    "bilstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
    "bilstm_model.add(layer = Dense(units = 100,  activation = 'softmax', name = '4th_layer'))\n",
    "bilstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
    "bilstm_model.add(layer = Dense(units = len(set(y_train)),  activation = 'sigmoid', name = 'output_layer'))\n",
    "# compiling the model\n",
    "bilstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 1976s 14s/step - loss: 0.6874 - accuracy: 0.5601 - val_loss: 0.6886 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "lstm_model_val_lstm_model_fit = bilstm_model.fit(X_claim, y_train,validation_split=0.1, batch_size= 64,epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 29s 715ms/step - loss: 0.6946 - accuracy: 0.5202\n",
      "Test set\n",
      "  Loss: 0.695\n",
      "  Accuracy: 0.520\n",
      "41/41 [==============================] - 29s 714ms/step - loss: 0.6846 - accuracy: 0.5666\n",
      "Test set\n",
      "  Loss: 0.685\n",
      "  Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "#bidirectional\n",
    "accr1 = bilstm_model.evaluate(X_claim_validation,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = bilstm_model.evaluate(X_claim_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model\n",
    "\n",
    "lstm_model = Sequential(name = 'lstm_nn_model')\n",
    "lstm_model.add(layer = Embedding(input_dim = max_features, output_dim = 120, name = '1st_layer'))\n",
    "lstm_model.add(layer = LSTM(units = 120, dropout = 0.2, recurrent_dropout = 0.2, name = '2nd_layer'))\n",
    "lstm_model.add(layer = Dropout(rate = 0.5, name = '3rd_layer'))\n",
    "lstm_model.add(layer = Dense(units = 100,  activation = 'softmax', name = '4th_layer'))\n",
    "lstm_model.add(layer = Dropout(rate = 0.5, name = '5th_layer'))\n",
    "lstm_model.add(layer = Dense(units = len(set(y_train)),  activation = 'sigmoid', name = 'output_layer'))\n",
    "# compiling the model\n",
    "lstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 640s 4s/step - loss: 0.6873 - accuracy: 0.5626 - val_loss: 0.6885 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "claim_val_lstm_model_fit = lstm_model.fit(X_claim, y_train,validation_split=0.1, batch_size= 64,epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 16s 383ms/step - loss: 0.6945 - accuracy: 0.5202\n",
      "Test set\n",
      "  Loss: 0.694\n",
      "  Accuracy: 0.520\n",
      "41/41 [==============================] - 16s 386ms/step - loss: 0.6846 - accuracy: 0.5666\n",
      "Test set\n",
      "  Loss: 0.685\n",
      "  Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "accr1 = lstm_model.evaluate(X_claim_validation,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = lstm_model.evaluate(X_claim_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIX-WAY CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10269, 520)\n",
      "(10269,)\n",
      "(1284, 520)\n",
      "(1284,)\n"
     ]
    }
   ],
   "source": [
    "print(X_claim.shape)\n",
    "print(swc_y_train.shape)\n",
    "print(X_claim_validation.shape)\n",
    "print(swc_y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        false\n",
       "1         true\n",
       "2         true\n",
       "3        false\n",
       "4         true\n",
       "5         true\n",
       "6        false\n",
       "7         true\n",
       "8         true\n",
       "9         true\n",
       "10        true\n",
       "11        true\n",
       "12       false\n",
       "13        true\n",
       "14       false\n",
       "15        true\n",
       "16        true\n",
       "17       false\n",
       "18        true\n",
       "19        true\n",
       "20       false\n",
       "21        true\n",
       "22        true\n",
       "23        true\n",
       "24       false\n",
       "25       false\n",
       "26        true\n",
       "27        true\n",
       "28       false\n",
       "29        true\n",
       "         ...  \n",
       "10239     true\n",
       "10240     true\n",
       "10241     true\n",
       "10242    false\n",
       "10243    false\n",
       "10244    false\n",
       "10245    false\n",
       "10246     true\n",
       "10247    false\n",
       "10248     true\n",
       "10249     true\n",
       "10250    false\n",
       "10251     true\n",
       "10252     true\n",
       "10253    false\n",
       "10254     true\n",
       "10255     true\n",
       "10256    false\n",
       "10257    false\n",
       "10258     true\n",
       "10259    false\n",
       "10260     true\n",
       "10261     true\n",
       "10262    false\n",
       "10263     true\n",
       "10264     true\n",
       "10265     true\n",
       "10266     true\n",
       "10267    false\n",
       "10268    false\n",
       "Name: label, Length: 10269, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swc_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_claim_test = pad_sequences(sequences = X_claim_test, maxlen = max_features, padding = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node Cast (defined at <ipython-input-21-89a4c57f24f3>:2) ]] [Op:__inference_train_function_4481]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-89a4c57f24f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m swc_claim_val_lstm_model_fit = lstm_model.fit(X_claim, swc_y_train,\n\u001b[1;32m----> 2\u001b[1;33m                                               validation_data = (X_claim_validation,swc_y_validation), epochs = 1)\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mswc_claim_test_lstm_model_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_claim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswc_y_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_claim_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mswc_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node Cast (defined at <ipython-input-21-89a4c57f24f3>:2) ]] [Op:__inference_train_function_4481]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# swc_claim_val_lstm_model_fit = lstm_model.fit(X_claim, swc_y_train,\n",
    "#                                               validation_data = (X_claim_validation,swc_y_validation), epochs = 1)\n",
    "# swc_claim_test_lstm_model_fit = lstm_model.fit(X_claim, swc_y_train,validation_data = (X_claim_test,swc_y_test), epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF FOR CLAIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for claims\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "claim_train_corpus = []\n",
    "for i in range(0, len(train_data['claim'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", train_data['claim'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    claim_train_corpus.append(new_data)\n",
    "    \n",
    "claim_val_corpus = []\n",
    "for i in range(0, len(validation_df['claim'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", validation_df['claim'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    claim_val_corpus.append(new_data)\n",
    "    \n",
    "claim_test_corpus = []\n",
    "for i in range(0, len(test_df['claim'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", test_df['claim'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    claim_test_corpus.append(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "vectorizer = TfidfVectorizer(max_features = 700)\n",
    "tfidf_x_train = vectorizer.fit_transform(claim_train_corpus).toarray()\n",
    "tfidf_x_train = pad_sequences(sequences = tfidf_x_train, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#tfidf validation data\n",
    "tfidf_x_val = vectorizer.fit_transform(claim_val_corpus).toarray()\n",
    "tfidf_x_val = pad_sequences(sequences = tfidf_x_val, maxlen = max_features, padding = 'pre')\n",
    "# tfidf_claim_val_lstm_model_fit = lstm_model.fit(tfidf_x_train, y_train,\n",
    "#                                                 validation_data = (tfidf_x_val,y_validation), epochs = 1)\n",
    "\n",
    "#tfidf test data\n",
    "tfidf_x_test = vectorizer.fit_transform(claim_test_corpus).toarray()\n",
    "tfidf_x_test = pad_sequences(sequences = tfidf_x_test, maxlen = max_features, padding = 'pre')\n",
    "#tfidf_claim_test_lstm_model_fit = lstm_model.fit(tfidf_x_train, y_train,\n",
    "#                                                validation_data = (tfidf_x_test,y_test), epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 725s 5s/step - loss: 0.6856 - accuracy: 0.5636 - val_loss: 0.6889 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "tfidf_claim_val_lstm_model_fit = lstm_model.fit(tfidf_x_train, y_train,validation_split=0.1, batch_size= 64,epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 21s 511ms/step - loss: 0.6960 - accuracy: 0.5202\n",
      "Test set\n",
      "  Loss: 0.696\n",
      "  Accuracy: 0.520\n",
      "41/41 [==============================] - 20s 480ms/step - loss: 0.6843 - accuracy: 0.5666\n",
      "Test set\n",
      "  Loss: 0.684\n",
      "  Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "accr1 = lstm_model.evaluate(tfidf_x_val,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = lstm_model.evaluate(tfidf_x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bidirectional\n",
    "\n",
    "lstm_model_val_lstm_model_fit = bilstm_model.fit(tfidf_x_train, y_train,validation_split=0.1, batch_size= 64,epochs = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZE FOR NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "tokenizer = Tokenizer(num_words = max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(texts = train_data['new'])\n",
    "X_train = tokenizer.texts_to_sequences(texts = train_data['new'])\n",
    "\n",
    "X_train = pad_sequences(sequences = X_train, maxlen = max_features, padding = 'pre')\n",
    "#validation\n",
    "tokenizer.fit_on_texts(texts = validation_df['new'])\n",
    "X_validation = tokenizer.texts_to_sequences(texts = validation_df['new'])\n",
    "X_validation = pad_sequences(sequences = X_validation, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#test\n",
    "tokenizer.fit_on_texts(texts = test_df['new'])\n",
    "test_text = tokenizer.texts_to_sequences(texts = test_df['new'])\n",
    "test_text = pad_sequences(sequences = test_text, maxlen = max_features, padding = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_claim_val_lstm_model_fit = lstm_model.fit(X_train, y_train,validation_split=0.1, batch_size= 10,epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr1 = lstm_model.evaluate(X_validation,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = lstm_model.evaluate(test_text,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 562s 2s/step - loss: 0.6865 - accuracy: 0.5601 - val_loss: 0.6987 - val_accuracy: 0.5202\n",
      "321/321 [==============================] - 545s 2s/step - loss: 0.6836 - accuracy: 0.5638 - val_loss: 0.6887 - val_accuracy: 0.5425\n"
     ]
    }
   ],
   "source": [
    "# val_lstm_model_fit = lstm_model.fit(X_train, y_train, validation_data = (X_validation,y_validation), epochs = 1)\n",
    "# test_lstm_model_fit = lstm_model.fit(X_train, y_train, validation_data = (test_text,y_test), epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_lstm_model_fit.history['accuracy'])\n",
    "print(val_lstm_model_fit.history['loss'])\n",
    "print(val_lstm_model_fit.history['val_accuracy'])\n",
    "print(val_lstm_model_fit.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF FOR NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf for new\n",
    "new_train_corpus = []\n",
    "for i in range(0, len(train_data['new'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", train_data['new'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    new_train_corpus.append(new_data)\n",
    "    \n",
    "new_validation_corpus = []\n",
    "for i in range(0, len(validation_df['new'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", validation_df['new'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    new_validation_corpus.append(new_data)\n",
    "    \n",
    "new_test_corpus = []\n",
    "for i in range(0, len(test_df['new'])):\n",
    "    new_data = re.sub('[^a-zA-Z]',\" \", test_df['new'][i])\n",
    "    new_data = new_data.lower()\n",
    "    new_data = new_data.split()\n",
    "    \n",
    "    new_data = [lemmatizer.lemmatize(word) for word in new_data if not word in stop_words]\n",
    "    new_data = ' '.join(new_data)\n",
    "    new_test_corpus.append(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321/321 [==============================] - 537s 2s/step - loss: 0.6861 - accuracy: 0.5621 - val_loss: 0.6939 - val_accuracy: 0.5202\n",
      "321/321 [==============================] - 541s 2s/step - loss: 0.6863 - accuracy: 0.5620 - val_loss: 0.6855 - val_accuracy: 0.5666\n"
     ]
    }
   ],
   "source": [
    "#tfidf\n",
    "vectorizer = TfidfVectorizer(max_features = 700)\n",
    "tfidf_x_train_new = vectorizer.fit_transform(new_train_corpus).toarray()\n",
    "tfidf_x_train_new = pad_sequences(sequences = tfidf_x_train_new, maxlen = max_features, padding = 'pre')\n",
    "\n",
    "#tfidf validation data\n",
    "tfidf_x_val_new = vectorizer.fit_transform(new_validation_corpus).toarray()\n",
    "tfidf_x_val_new = pad_sequences(sequences = tfidf_x_val_new, maxlen = max_features, padding = 'pre')\n",
    "#tfidf_new_val_lstm_model_fit = lstm_model.fit(tfidf_x_train_new, y_train,\n",
    "#                                               validation_data = (tfidf_x_val_new,y_validation), epochs = 1)\n",
    "\n",
    "#tfidf test data\n",
    "tfidf_x_test_new = vectorizer.fit_transform(new_test_corpus).toarray()\n",
    "tfidf_x_test_new = pad_sequences(sequences = tfidf_x_test_new, maxlen = max_features, padding = 'pre')\n",
    "#tfidf_new_test_lstm_model_fit = lstm_model.fit(tfidf_x_train_new, y_train,\n",
    "#                                                validation_data = (tfidf_x_test_new,y_test), epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_claim_val_lstm_model_fit = lstm_model.fit(tfidf_x_train_new, y_train,validation_split=0.1, batch_size= 10,epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr1 = lstm_model.evaluate(tfidf_x_val_new,y_validation)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))\n",
    "accr2 = lstm_model.evaluate(tfidf_x_test_new,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr2[0],accr2[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
